{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a62674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from data_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3409a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438b447",
   "metadata": {},
   "source": [
    "Below is a construction of the dictionary of basis functions. The mauna_loa training dataset has sinusoidal behaviour with an increasing, seemingly linear, vertical offset. Thus I have decided to use a combination of polynomial, sine, and cosine basis functions.\n",
    "\n",
    "As polynomial regression has a tendency to overfit with higher order terms, I will use polynomial terms up to order 5 (??? should change this bc answer uses this... ???). As we are asked to have at least 200 basis functions, I will also use 100 sine functions and 100 cosine functions.\n",
    "\n",
    "The sine and cosine functions increasing frequencies, i.e., they have frequencies of (w)(pi) with increasing w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec33ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = np.pi\n",
    "max_order = 5\n",
    "max_w = 100\n",
    "\n",
    "# Dict has 211 basis functions\n",
    "Dict = []\n",
    "\n",
    "p = [p for p in range(0,max_order+1)]\n",
    "Dict += [lambda x, i=i: x**i for i in p]\n",
    "\n",
    "w = [w for w in range(1,max_w+1)]\n",
    "Dict += [lambda x, i=i: np.sin(i*PI*x) for i in w]\n",
    "Dict += [lambda x, i=i: np.cos(i*PI*x) for i in w]\n",
    "\n",
    "# https://www.reddit.com/r/learnpython/comments/zajla6/how_to_return_a_list_of_lambda_function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c16fda7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (4204735121.py, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 79\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"Iteration \", k\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "# this stuff needs to be tested!!! --> working!!!\n",
    "\n",
    "def J(phi_i, r_k):\n",
    "    # phi_i is the ith column of the PHI matrix\n",
    "    # where the ith column of the PHI matrix contains the mapping\n",
    "    # of each training point x_{1-N} to the feature phi_i\n",
    "    # the ith row of the PHI matrix is the mapping of x_i to all phi_{1-M}\n",
    "    # i.e., PHI_ij = \n",
    "    return np.square(phi_i.T.dot(r_k)) / phi_i.T.dot(phi_i)\n",
    "\n",
    "def mdl(N, l2_loss, k):\n",
    "    return N/2 * np.log(l2_loss) + k/2 * np.log(N)\n",
    "\n",
    "def greedy_regression_model(x_train, y_train, epsilon_r, testing=False):\n",
    "    N = x_train.shape[0]\n",
    "    k = 0\n",
    "    I_selected = []\n",
    "    I_candidates = np.arange(0,len(Dict))\n",
    "    r_k = y_train # residual/training error vector\n",
    "\n",
    "    MDL = np.inf\n",
    "    # MDL will decrease as model complexity grows and then increase as overfitting occurs\n",
    "    # so once MDL stops decreasing, we will stop the algorithm (stop at some relative growth, epsilon_r)\n",
    "    # (or until no more basis functions to select)\n",
    "    \n",
    "    weights = None\n",
    "    chosen_k = 0\n",
    "    \n",
    "    if testing:\n",
    "        print(\"Initial:\")\n",
    "        print(\"I_selected: \", I_selected)\n",
    "        print(\"I_candidates: \", I_candidates)\n",
    "        print(\"MDL: \", MDL)\n",
    "    \n",
    "    while I_candidates.shape[0] > 0:\n",
    "        # 1) increment k\n",
    "        #    k is also equal to num fcns in the model after this iteration\n",
    "        k += 1\n",
    "        \n",
    "        # 2) pick new basis fcn from Dict\n",
    "        #    use orthogonal matching pursuit metric\n",
    "        #    i.e., pick basis fcn from candidates that maximmizes J\n",
    "        Js = []\n",
    "        for i in I_candidates:\n",
    "            Js.append(J(np.array(Dict[i](x_train)), r_k))\n",
    "        i_k = I_candidates[np.argmax(Js)]\n",
    "        \n",
    "        # 3) add selected basis fcn to I_selected\n",
    "        I_selected.append(i_k)\n",
    "        \n",
    "        # 4) delete selected basis fcn from I_candidates\n",
    "        I_candidates = np.delete(I_candidates, i_k)\n",
    "        \n",
    "        # 5) use current I_selected model to solve for weights\n",
    "        #    use SVD to compute weights\n",
    "        PHI = np.empty((N, k))\n",
    "        for i, fcn in enumerate(I_selected):\n",
    "            PHI[:,i] = Dict[fcn](x_train).reshape(PHI[:,i].shape)\n",
    "\n",
    "        U, sigma, V_t = np.linalg.svd(PHI, full_matrices=False, compute_uv=True, hermitian=False)\n",
    "        weights = np.linalg.multi_dot([V_t.T, np.linalg.inv(np.diag(sigma)), U.T, y_train])\n",
    "        \n",
    "        # 6) update residual/training error vector\n",
    "        y_hat = PHI.dot(weights)\n",
    "        r_k = y_train - y_hat\n",
    "        \n",
    "        # 7) check MDL stopping criterion\n",
    "        temp = mdl(N, np.sqrt(np.sum(np.square(r_k))), k)\n",
    "        done = False\n",
    "        if temp >= MDL:\n",
    "            if abs(temp - MDL)/abs(MDL) > epsilon_r:\n",
    "                done = True\n",
    "        else:\n",
    "            MDL = temp\n",
    "            chosen_k = k\n",
    "            \n",
    "        if testing:\n",
    "            print(\"\\n\")\n",
    "            print(\"Iteration \", k\n",
    "            print(\"chosen index: \". i_k)\n",
    "            print(\"I_selected: \", I_selected)\n",
    "            print(\"I_candidates: \", I_candidates)    \n",
    "            print(\"PHI shape: \", PHI.shape)\n",
    "            print(\"prev MDL: \", MDL)\n",
    "            print(\"MDL: \", temp)\n",
    "            print(\"chosen_k: \", chosen_k)\n",
    "            \n",
    "            plt.plot(x_train, y_train, label=\"actual\")\n",
    "            plt.plot(x_train, y_hat, label=\"pred\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.show()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if testing:\n",
    "        print(\"\\nFinal:\")\n",
    "        print(\"num fcns selected: \", len(I_selected))\n",
    "        print(\"chosen k: \", chosen_k)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return I_selected[:chosen_k], weights[:chosen_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958c902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('mauna_loa')\n",
    "x_train = np.vstack([x_valid, x_train])\n",
    "y_train = np.vstack([y_valid, y_train])\n",
    "\n",
    "epsilon_r = 0.1\n",
    "\n",
    "I_selected, weights = greedy_regression_model(x_train, y_train, epsilon_r, testing)\n",
    "print(\"num features: \", len(I_selected))\n",
    "print(\"num weights: \", len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d061f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI = np.empty((x_test.shape[0], len(I_selected)))\n",
    "for i, fcn in enumerate(I_selected):\n",
    "    PHI[:,i] = Dict[fcn](x_test).reshape(PHI[:,i].shape)\n",
    "\n",
    "y_predict = PHI.dot(weights)\n",
    "RMSE = np.sqrt(np.mean(np.square(y_test-y_predict)))\n",
    "\n",
    "print(\"RMSE = \", RMSE)\n",
    "\n",
    "plt.plot(x_test, y_test, label=\"actual\")\n",
    "plt.plot(x_test, y_predict, label=\"pred\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
